# ğŸ“ˆ Customer Lifetime Value (CLV) Prediction â€“ End-to-End ML Pipeline

Predict **customer spending for the next 30 days** using real-world retail transaction data and a production-ready machine learning pipeline.

This project demonstrates **end-to-end data engineering, feature engineering, model training, evaluation, and deployment** using modern Python ML tooling.

---

## ğŸš€ Project Overview

**Goal:**
Predict how much a customer is likely to spend in the **next 30 days**, based on their historical behavior.

**Why it matters:**

* Helps businesses target high-value customers
* Enables smarter promotions and retention strategies
* Supports demand forecasting and revenue planning

This repository covers the **full ML lifecycle**, not just modeling.

---

## ğŸ§  Key Highlights

* âœ”ï¸ Realistic **synthetic data generation** (customers, transactions, products)
* âœ”ï¸ Robust **EDA & data quality checks**
* âœ”ï¸ Business-safe **data cleaning strategies**
* âœ”ï¸ Advanced **feature engineering (RFM, trends, momentum)**
* âœ”ï¸ Time-aware **30-day target creation (no data leakage)**
* âœ”ï¸ Gradient Boosting regression model
* âœ”ï¸ Full **FastAPI inference service**
* âœ”ï¸ **Streamlit-ready deployment**

---

## ğŸ—‚ï¸ Repository Structure

```text
â”œâ”€â”€ Datageneration&Eda.ipynb        # Data generation + EDA
â”œâ”€â”€ Model_Building.ipynb            # Feature engineering + modeling
â”œâ”€â”€ app.py                          # FastAPI inference API
â”œâ”€â”€ best_customer_spend_model.pkl   # Trained ML pipeline
â”œâ”€â”€ requirements.txt                # Dependencies
â”œâ”€â”€ README.md                       # Project documentation
â”œâ”€â”€ demo.png                        # Demo screenshot
â””â”€â”€ LICENSE
```

---

## ğŸ§¾ Dataset Description

Synthetic retail data is generated to closely mimic real production systems:

### Tables Used

* **customer_details**
* **store_sales_header**
* **store_sales_line_items**
* **products**

### Data Characteristics

* Multiple transactions per customer
* Transaction â†’ line-item hierarchy
* Temporal consistency enforced
* Intentional real-world issues (nulls, duplicates, outliers)

---

## ğŸ” Exploratory Data Analysis (EDA)

Key checks performed:

* Row counts & schema validation
* Null value distribution
* Duplicate detection
* Date range validation
* Transaction consistency (header vs line items)
* Customer repeat behavior

ğŸ“Œ **Purpose:**
Identify data quality issues early and design robust cleaning rules.

---

## ğŸ§¹ Data Cleaning Strategy

Handled **real-world data problems** safely:

* Null values â†’ business-aware imputation
* Duplicate records â†’ deduplication using PKs
* Invalid foreign keys â†’ removed safely
* Monetary mismatches â†’ recomputed transaction totals
* Date issues â†’ removed future or inconsistent dates
* Outliers â†’ capped (not removed) to preserve high-value customers
* Cold-start customers â†’ explicitly modeled

---

## ğŸ§© Feature Engineering

Customer-level features aggregated from historical data:

### Core Features

* Total spend
* Total transactions
* Average order value
* Recency & tenure

### Time-Window Features

* Spend & transactions (31â€“60 days)
* Spend & transactions (61â€“120 days)

### Advanced Features

* Spend velocity
* Transaction density
* Momentum ratios
* Loyalty efficiency
* Cold-start indicator

ğŸ¯ **Target Variable:**
`target_30d_spend` â†’ Total spend in the **next 30 days**

---

## ğŸ¤– Model Training

### Models Evaluated

* Baseline (Last 30-day spend)
* Linear Regression
* Gradient Boosting Regressor âœ…

### Final Model

* **Gradient Boosting Regressor**
* Full preprocessing pipeline (imputation, scaling, encoding)
* Time-based train/validation split

### Metrics Used

* MAE
* RMSE
* RÂ² Score

---

## ğŸ“¦ Model Serialization

The **entire sklearn pipeline** (preprocessing + model) is saved using `joblib`:

```python
joblib.dump(model, "best_customer_spend_model.pkl")
```

This ensures **training-serving consistency** in production.

---

### Endpoints

* `GET /` â†’ Health check
* `POST /predict` â†’ Predict 30-day customer spend

### Features

* Model loaded safely at startup
* Strict input schema using Pydantic
* Handles unseen categorical values
* Ready for deployment

---

## ğŸŒ Streamlit Deployment

The model and API are designed to be **easily deployed using Streamlit** for interactive demos.

### Typical Streamlit Flow

* User inputs customer features
* Calls FastAPI `/predict` endpoint
* Displays predicted 30-day spend

ğŸ“Œ This makes the project **demo-ready for interviews and showcases**.

---

## ğŸ–¼ï¸ Demo

Below is a sample demo of the application in action:

![Demo Screenshot](demo.png)

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/<biswasudipta>/<30days_spend_predictor>.git
cd <repo-name>
```

### 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Run FastAPI

```bash
uvicorn app:app --reload
```

Open:

```
http://127.0.0.1:8000/docs
```

---



---

## ğŸ§  Skills Demonstrated

* Data Engineering
* Feature Engineering
* Time-series aware ML
* Model evaluation
* Production best practices

---

## ğŸ“Œ Future Improvements

* Add Docker support
* Deploy on cloud (Azure / AWS)
* Batch prediction endpoint
* Model monitoring & drift detection
* Experiment tracking (MLflow)

